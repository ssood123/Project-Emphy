# Team 7: Emphy Software

We used different design methods to create two versions of an indoor navigation robot. Our first prototype was the Arduino Bot and all of the code for the arduinobot is in its relevant folder. The code for the final OpenBot based prototype is placed in the respecrtive folder.

## Emphy OpenBot

For this prototype, we utilized the opensource project, OpenBot, hence the name. [OpenBot](https://www.openbot.org/) is an opensource project currently in development which aims to utilize the processing power of phones to create small autonomous navigation cars for indoor environments. These robots are capable of both autonomous navigation and can also be controlled by another device through TCP communication over the same network. There are 3 major programmable components in this design: An Android phone running the OpenBot application, a raspberry pi which acts as a smart controller and an arduino nano which controls the hardware and motors of the robot.

#### OpenBot application

The opensource project consists of two components. There is an android phone which runs the Openbot application. The andoid application uses the TensorFlow Lite Object Detection Android Demo to train a nerual network to detect walls and obstacles using images from the phones camera. The data collected from the phone is then communicated over to an Arduino Nano. The arduino nano controls the four motors of the robot car through a motor driver. The nano is also connected to sensors such as an ultrasonic sensor, speed encoders and LED indicators. Newer sensors are also being added to this project which were not used in Emphy. Both of these components communicate with each other via serial link. The raspberry pi also communicates with the android phone running the OpenBot application. 

To install the OpenBot application, the apk for the app can be found [here](https://github.com/intel-isl/OpenBot/tree/master/android). This app is only available for android phones for now since it is built using Android APIs. The github contains code for two applications, the main app and a controller app. We just use the main app since our raspberry pi will act as a controller.

The Software to flash the arduino nano is also available through OpenBot and can be found [here](https://github.com/intel-isl/OpenBot/tree/master/firmware). The code needs to be edited to indicate what sensors have been implemented on the OpenBot.

*The setup for these if straighforward and does not need any changes from the official version for our project as of April 2021. Future updates may change how the raspberry pi can connect with the phone.

#### Raspberry Pi

The setup for the raspberry pi is relatively simple. It can be set up with Raspbian OS in any preferred method. Once connected to the Pi, it can be connected to the network to which the andorid phone will be connected to and this network setting can be saved. Once connected to the internet, the following commands should be run on the raspberry pi.

1. *sudo apt-get update*
2. *sudo apt-get upgrade*
3. *pip3 install zeroconf*
4. *pip3 install SpeechRecognition*
5. *pip3 install espeak*

The zeroconf module allows for discovery of the phone on the network, the SpeechRecognition allows for speech to text conversion and espeak allows for text to speech audio output. 

Once these packages are installed and the leash and headset are connected as noted in the hardware readme, Emphy.py can be run on the raspberry pi. Emphy.py starts off by initializing the mic, setting up all variables and initializing its current coordinates and direction which are hardcoded into the code in the main part of the file. It then has a blocking wait until it is connected to a phone running the OpenBot application. make sure that the application is trying to connect to a phone controller through the OpenBot application UI. Once connected, the code asks the user for their destination. Using speech to text, the input audio is converted into text and the command is searched through to see if any destination was mentioned. A future version of the project could use smart communication to comprehend user commands rather than just searching for a mention of a destination by the user. Once it gets the destination coordinates, it converts it into instructions such as turn left, right or straight for *x* feet. The code then cycles through these instructions until it reaches the end. It updates its current location to the previous destination and it starts all over by asking the user for a new destination. In our current prototype, the robot assumes it has made a 90 degree turn and has moved the correct amount of distance. There is no way for it to confirm if it has made the correct turns and the method by which it currently makes turns, is also inaccurate. Future versions can use gyroscopes and better localization for more precise commands.

The leash buttons have been set up as GPIO interrupts on the Pi. When the buttons are pressed, the instructions are paused at their current stage until the pause button is unpressed. For the stop button, it ends the loop at the current point, adjusts its starting location to the current location and asks the user for input again. The vibration motors are simply set up as GPIO outputs and the audio output is done through the speakout method in Emphy.py. 

#### Insturction generation code
This part explains the code from instructionGneration.py file. Note in the file there is a map and a function that called for pathfinding library, that is only for testing the instruction generation code and is not used on the phone-controlled bot.

The code has two main part. First it generates a list of instruction for every grid that the robots move to. The program will compare the coordinates of its current location and the next coordinate it needs to move to, along with the direction the robot is currently facing, it will determine how the robot get to the next coordinate. The outer if statements check the position the next coordinate is relative to the next coordinate (whether it is on the North, West, East, or South direction); the inner if statements check the robot is currently facing and will determine which side it need to turn to (left, right, or turn back) or if should go straight. For example, if the robot is currently facing North, and the next coordinate is on the West direction, the robot needs to turn left to get to the next coordinate.  The resulting list from the first part of the program will look something similar to {(“Start”, “N”), (“A”, “N”), (“L”, “W”)}, except for the first tuple, which is the starting point and the direction the robot initially is facing,  the first character in the tuple is for the action the robot should be taking (L for turning left, R for turning right, B for a U-turn, A for going straight) and the second character is the direction the robot will be facing after complete that movement.  

The second part of the code is to compress the first list so it would not out put instruction for every grid. The program will add the distance the robot should be going straight, and if there is any turn, add that to the output list, and then starts counting distance for going straight over again. The output is still a list, and it looks something like {20, “R”, 11, “L”}. Note that in this list there is no tuple, the numbers represent the distance the robot should go straight. It is using the same unit for each grid, for example, if the grid is 2 feet each, the 20 means going straight for 40 feet. Any character in the list represents a turn (L for left, R for right, B for U-turn).  This is the final output of the instruction generation program, and the resulting list will be used to control the robot movement.

#### Pathfinding code
This part explains the code from AStarPathfinding.py.

As an overview, this program uses the A* pathfinding algorithm. We have decided to use it because it is fast due to its use of the priority queue. Also, in order to make the pathfinding work, we have to implement a map of the robot's environment.

Here is some background information about the map: We decided to implement a map of the second floor of Photonics. This map is stored as a 2D Python array of 1s and 0s where 0s represent free space and 1s represent walls/obstacles. It is stored in the variable “grid”, which can be found in the function findPath(). In the findPath() function, the rows variable stores the size of the map (the number of rows and columns of the 2D array). In this case, it’s 30. This variable allows the “grid” variable to store an initialized array of the determined size where every element in the array is initialized to 0. Then, the function includePredefinedMap() actually sets the walls/obstacles in the array, meaning that some elements in the “grid” variable get set to a 1. After, the map is ready to be used for pathfinding.

The program starts at the main() function. There is a variable called listOfSpecialPoints. These are specific destinations that we determined in the second floor of photonics that are implemented into the map. The user can go to any of these destinations. For the purpose of this code, the starting position is “Water”, and the destination is “Classroom”. When the main function finishes, it calls findPath() and passes the coordinates of the starting position and the destination. findPath() first initializes the map in the manner described earlier. Then, the AStarAlgorithm() function is called and it takes as arguments the map (the 2D array), the starting position coordinate, and the destination's coordinate. This function calculates the shortest path from the starting position to the destination using the A* pathfinding algorithm while taking into account the walls and obstacles. After it's done, it stores the calculated path in the variable generatedPath, which is a global variable declared at the top of the program file. More specially, generatedPath gets a list of coordinates that represent the path from the starting coordinate to the destination. It is worth noting that the path is guaranteed to have a minimal number of turns, which means no zig-zags. After this is done, the “grid” variable is printed with the AStarAlgorithm function’s path superimposed on it. This path is represented as Xs.
 
Finally, generatedPath gets slightly modified and gets passed to the MakeInstruction() function. This function does what is described in the “Instruction Generation code” section of this Readme.md file.

### Map Creation code

This part describes MapCreation.py.

As an overview, the EMPHY robot has to store an indoor map of its environment to be able to navigate via pathfinding. As described in the pathfinding section of this Readme.md file, the map concept is simple. However, creating it may be difficult because one would have to determine the exact coordinates of walls/obstacles and free space. This is especially true if the array size is big. The purpose of MapCreation.py is to facilitate the process of map creation.  

The program allows the user to first specify the number of rows and columns of the map. Then, the program initializes a 2D array with the specified number of rows and columns. At the same time, the program opens a Pygame window which has a grid of squares. The grid has the number of rows and columns the user specified and each square corresponds to an element in the 2D array that is the map. Initially, each square in the Pygame window represents free space and is white. At the same time, each element in the 2D array map is a 0. If a user clicks a square, it becomes black (which represents an obstacle/wall) and the corresponding element in the 2D array goes from 0 to 1. The user can also change a black square to white so the corresponding element in the 2D array goes back to 0. After the user finishes creating the map, he or she presses space to print the map as a 2D Python array and a 2D C++ array. There are two different arrays because we have two versions of EMPHY – an Arduino bot (C++ array) and the EMPHY OpenBot (Python array). The user can then copy and paste the printed maps to another relevant program. Also, if the user selected that he or she wanted to save images of the map to the desktop, then a picture of the current Pygame window is saved to the user's desktop every time he or she presses space. 


