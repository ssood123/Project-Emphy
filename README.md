# 21-07-Emphy

EMPHY is a cost-effective robot that is capable of safely guiding a visually impaired person both at home or through large indoor environments like hospitals, airports and schools. Feedback will be provided through the use of haptic vibrations, so the robot can guide and be guided by the user’s hands, similar to how a guide dog provides guidance. The EMPHY robot will store an indoor map of the environment and be able to calculate the safest path from its current location to the desired destination, taking into account user preferences and input by voice command (“Take me to the Auditorium” or "Water Fountain"). In addition, while the robot is moving towards a destination,  it is able to detect and maneuver around obstacles. All of this can be done without the use of an expensive building-wide sensor network and cost-prohibitive sensors and computation. 


## Emphy Overview

Throughout the 1 year long project of creating a robot that could help users in indoor navigation we were able to create two different prototypes of the robot. One protype was using the OpenBot open source which was a cheaper version and had better navigation. It did come with some challenges which is discussed below which is why we pivoted to another prototype which was an Arduino based robot which was more simpler but also less powerfull that the OpenBot in terms of proceesing power and accuracy in navigation. Finally by the end of the semester we were successfuly able to implement both versions of the robot and demo them which was a huge accomplishment for the team. We discuss about the OpenBot version of the robot in great detail in our readme's, the implementation and challanges that were faced along the way. The instructions to build and run the robot can be found in the [Software README](https://github.com/ssood123/Senior-Design-Project/blob/main/README_Software.md) and the [Hardware README](https://github.com/ssood123/Senior-Design-Project/blob/main/README_HARDWARE.md)


#### Overview

Currently our OpenBot is able to take in a destination input via voice command and able to create a path from the starting location using it's Pathfinding algorithm. The OpenBot is connected to a leash. The leash accepts 2 user inputs. One input is for entering the destination through **voice commands** and the other one is a **start/stop** button incase the user needs to take a break or some other problem arises. Our robot is also equipped with autonomous navigation which comes with the OpenBot software and helps in the navigation aspect of our project. A rasberry pi is connected to the OpenBot which acts like a "*control unit*" handling the communication of the leash and also the pathfinding algorithm. At a high level the OpenBot consists of 4 main components. The first is the OpenBot car itself which has been 3-D printed from the open source environment of OpenBot. The second component is the raspberry pi which acts as a control unit communicating with the OpenBot. The third component is the mobile device with the OpenBot application installed in it which is responsible for handling alot of the processing that the OpenBot does. The last component is the leash which is a direct connection betweeen the user and the robot. A headphone is connected to allow the user to communicate with EMPHY and also for EMPHY to provide updates in terms of direction and distace.

#### Challenges

The first major challenge was to make changes to the OpenBot application. While OpenBot allows for autonomous navigation, it has no sense of where it is in an environment. It needed a controller, the raspberry pi, to help the openBot navigate to a destination. This required communication between the pi and android application but when we were in the initial stages of development, the android application could only be controlled by a PS4 controller over bluetooth. Seccessive updates allowed control from another phone controller application, however this still utilized android Nearby Connections, somethign not available on the pi. Due to the complexity of the OpenBot application, there was a point of time when our team hit a snag and had difficulty in changing any of the openbot application to allow pi-android communication. This is the reason we started work on the ArduinoBot. Luckily, an update to the application in March changed the communication method from bluetooth to TCP communication. issue was resolved with an update that helped in making changes to the OpenBot software uch easier. We were finally able to get the OpenBot to communicate with the rasberry pi and implement our software into the OpenBot which is discussed in greater details in the software section.

The second challenge is defintely the voice comands. At times the voice recognition lags or has a hard time understanding different accents which defintely needs to be worked on in the future to provide a very simple user experience not causing any additional problems to the EMPHY users.

The third challenge was getting Emphy to stay on the staright path as in some of our earlier testings it kept diverting ots path from the straight line. So localization was one of the issues we were tackling. This was solved using encoders though there is defintely scope of improvements to again provide a seamless user experience to EMPHY users. Additionally turning was also an issue which needs to be worked on by future EMPHY engineers!
